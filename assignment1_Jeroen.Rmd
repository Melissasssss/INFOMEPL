---
title: "Lab assignment 1: t-tests, linear regression, analyzing lexical decision task"
author: "INFOMEPL"
date: "Deadline: Tuesday, February 27"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## General lab information

This assignment contains 10 questions: 9 group questions and 1 individual question. For each question you can receive up to 1 point. The group questions should be submitted by the whole group to Blackboard and answers to individual questions should be submitted via a Blackboard quiz. If the question does not specify what type of question it is, it is a group question.

More details on the whole procedure:

You work in groups of 2 or 3 on these exercises. We expect you to work on this during class time so that you can work with your group and ask questions to a teacher/teaching assistant. It is not acceptable to miss these classes without agreement from your group, or to repeatedly miss classes. If your group members miss lab classes without agreement from your group, please inform your teacher. It is up to you, however, to decide whether you work on location or on-line during labs.

We suggest all group members doing these exercises on their individual computers simultaneously: this improves (student) learning and also makes it easier to find mistakes. Don't rely on other group members' answers if you don't understand why they are correct: this is meant to be an interactive collaboration with your group, so ask your group members to explain. If your group gets stuck on a question or you can't agree on an answer, ask for help from your TA. If you meet online, please share your video if bandwidth and circumstances allow, this makes for a more personal conversation.

The assignment has to be submitted on Blackboard by *Tuesday, 6pm, February 27*. The individual question also has to be submitted by the same deadline.

In group questions, it is generally best to start by asking every group member's opinion and then work on a written answer together. You can and should also ask your teacher to help you whenever you get stuck or are not sure about something.

Many questions build on previous questions being completed correctly, so you should be confident of your answer before using it in further questions. If you get stuck and the teacher can't get help immediately, you can move on to the next topic until your teacher can help.

## Introduction

In this assignment, you will work with data from an auditory lexical decision task. In this task, participants listen to the speaker who says a word/pseudoword and have to decide whether what they just heard was a word (by pressing one key) or not (by pressing another key). We collect their responses. Two measures are of interest: reaction times (how much time it took them to respond) and accuracy (was their response correct or not?). More details about the task, why it is interesting, what it can reveal about the organization of the lexicon in our mind etc. can be found in the paper Tucker et al. (2019), attached to this assignment.

Your task will be to analyze selected data.

### What will you hand in?

You will hand in two files: an html/pdf file and an R file (or an html/pdf file and a rnw/rmd file, if you use knitr). 

For the ease of cross-reference, the html/pdf file should include the same code as the R file or refer to relevant snippets (e.g., it could say, see code in part 1 and in the R code, a comment signals where part 1 starts and ends). If you include the R code in your html/pdf file, you can do so by either copy-pasting your code into the file, or, better, you could use an engine for dynamic report with R like knitr (link: https://yihui.org/knitr/) -- that is, in fact, the way that this report is created.

The R (rnw/rmd) file should run without errors if we run it top to bottom and all the code you used to answer the questions should be present, from loading the csv files up to the analysis required of you in questions (if there are errors we will deduct points). You don't have to include the csv files that you received from us but use the same naming conventions for them as we do in section Data preparation below.

Apart from handing in the R and the html/pdf file, you have to respond to the individual question on Blackboard.

### What can you use?

You can use R and any packages you find useful (unless some questions explicitly prohibit that). Some packages are even recommended to use - you get a hint to use them in questions. You also can (and should) reuse the code present in these assignment instructions. For an ease of reuse, we put the code separately into an Rmd file.

It is also allowed to use AI tools (like ChatGPT) during the assignment. If you use such tools, do acknowledge the use of those tools in your answers (unacknowledged use counts as fraud).

## Data preparation

We start by loading useful packages (dplyr for data manipulation and ggplot2 for graphics) and loading data as two data frames and checking the structure of the data frames.

```{r}

library(dplyr)
library(ggplot2)

itemdata <- read.csv("MALD1_SelectedItemData.csv", sep="\t")
# This data frame stors information about items
str(itemdata)
head(itemdata)

responsedata <- read.csv("MALD1_SelectedResponseData.csv", sep="\t")
# This data frame stors information about participants' responses
str(responsedata)
head(responsedata)

```

Afterwards, we visually check one variable (RT=reaction times). (The sample command below selects 10,000 random samples from the data set; if we did not do it, the graph would be too big and would take too much time to load.)

```{r}

# we plot 10,000 randomly selected samples from responsedata
g1 <- ggplot(responsedata[sample(1:length(responsedata$RT), 10000),], aes(RT))

# we specify that the plot should be a bar plot.
g1 <- g1 + geom_bar()

g1

```

The data frame responsedata includes responses. The following columns are relevant: Item = what word/pseudoword was tested; RT = reaction times to the item; ACC = accuracy (TRUE - correct response, i.e., *yes* for a word, *no* for a pseudoword; FALSE - incorrect response). The data frame itemdata includes information about individual items, including frequencies based on several different corpora, stress patterns of words, whether the element is a word or not, average distance from words based on phonological Levensteihn distance etc. For detailed descriptions of what columns in these files mean, check Tucker et al. (2019).

### Q1: Merge data and report basic information

Merge the two data frames (responsedata and itemdata) into one data frame. Afterwards, report descriptive summaries for the whole dataset and also separately for Subjects 15351, 16854 and 170373. Report summaries of the following measures:

1. Reaction times (RT)
2. Frequencies (use FreqCOCAspok, which is a spoken corpus of American English). 
3. RT for words and pseudowords separately. Use the variable IsWord: when it is TRUE, the word is a real word, when it is FALSE it is a pseudoword. 

For descriptive summaries it is enough you report means and spread of data - e.g., variances or standard deviations, or just a range of data. You can also provide histograms (for example, by replicating the code we have above; for RTs this is already done).

```{r}

# Hint: try to use dplyr and a family of join functions,  and group_by and summarise from the same package.
# Of course, other functions might be useful and needed.

# Check help of these functions and check dplyr for details.

# Merge data frames by item column which belongs to both data frames
merged_data <- merge(responsedata, itemdata, by = "Item")

# Overall descriptive summaries OLD WAY, gave error
# overall_summaries <- merged_data %>%
#   summarise(
#     MeanRT = mean(RT, na.rm = TRUE),
#     SdRT = sd(RT, na.rm = TRUE),
#     RangeRT = range(RT, na.rm = TRUE),
#     MeanFreqCOCAspok = mean(FreqCOCAspok, na.rm = TRUE),
#     SdFreqCOCAspok = sd(FreqCOCAspok, na.rm = TRUE),
#     RangeFreqCOCAspok = range(FreqCOCAspok, na.rm = TRUE)
#   )

# Overall descriptive summaries using range function to only get 1 row per summaries group
overall_summaries <- merged_data %>%
  summarise(
    MeanRT = mean(RT, na.rm = TRUE),
    SdRT = sd(RT, na.rm = TRUE),
    MinRT = min(RT, na.rm = TRUE),
    MaxRT = max(RT, na.rm = TRUE),
    MeanFreqCOCAspok = mean(FreqCOCAspok, na.rm = TRUE),
    SdFreqCOCAspok = sd(FreqCOCAspok, na.rm = TRUE),
    MinFreqCOCAspok = min(FreqCOCAspok, na.rm = TRUE),
    MaxFreqCOCAspok = max(FreqCOCAspok, na.rm = TRUE)
  )

# RT for words and pseudowords
word_pseudoword_summary <- merged_data %>%
  group_by(IsWord) %>%
  summarise(
    MeanRT = mean(RT, na.rm = TRUE),
    SdRT = sd(RT, na.rm = TRUE),
    MinRT = min(RT, na.rm = TRUE),
    MaxRT = max(RT, na.rm = TRUE)
  )

# For specific subjects
subjects_summary <- merged_data %>%
  filter(Subject %in% c(15351, 16854, 170373)) %>%
  group_by(Subject) %>%
  summarise(
    MeanRT = mean(RT, na.rm = TRUE),
    SdRT = sd(RT, na.rm = TRUE),
    MinRT = min(RT, na.rm = TRUE),
    MaxRT = max(RT, na.rm = TRUE),
    MeanFreqCOCAspok = mean(FreqCOCAspok, na.rm = TRUE),
    SdFreqCOCAspok = sd(FreqCOCAspok, na.rm = TRUE),
    MinFreqCOCAspok = min(FreqCOCAspok, na.rm = TRUE),
    MaxFreqCOCAspok = max(FreqCOCAspok, na.rm = TRUE)
  )

# View the data frames
View(overall_summaries)
View(subjects_summary)
View(word_pseudoword_summary)

# Histogram for Response times (overall)
ggplot(merged_data, aes(x = RT)) +
  geom_histogram(binwidth = 100, fill = "blue", color = "black") +
  labs(title = "Histogram of Reaction Times", x = "Reaction Time (ms)", y = "Frequency") +
  theme_minimal()


```

## Cohen's d

If everything was done correctly, you should have found out that the distribution of RTs differs depending on whether people responded to a word or to a pseudoword. This might make sense - it seems that responding to a pseudoword takes more time. We will now be investigating this effect further. Roughly, we want to address the following question: can we conclude with reasonable confidence that responses to words are faster than responses to pseudowords? We will further qualify and specify this question as we proceed.

We will start by investigating Cohen's $d$. This is a standardized measure of effect size: it measures the strength of difference between two means. The formula is calculated as follows:

$$d = \frac{\bar{x_1} - \bar{x_2}}{s}$$

where $\bar{x_1}$ is the mean of data $x_1$ (i.e., RTs for words) and $\bar{x_2}$ is the mean of data for $x_2$ (RTs for non-words) and $s$ is a pooled standard deviation, which has been calculated as shown below in Cohen's original work (note: there are various ways of calculate $s$, we will use this one).  Assume that there are $n_1$ observations for $x_1$ and $n_2$ observations for $x_2$, i.e., $|x_1|=n_1$ and $|x_2|=n_2$. $var(x)$ is the variance of $x$ (you can get it in \textbf{R} by using the function \textbf{var}). Then:

$$s=\sqrt{\frac{(n_1-1)\cdot var(x_1) + (n_2-1)\cdot var(x_2)}{n_1+n_2-2}}$$

This formula can be simplified if the length of $x_1$ is equal to the length of $x_2$, i.e., $n_1=n_2$, which is the case for all our computations below. We leave it to you to make the algebraic simplifications. You can make this simplification and use it - throughout this assignment, it will be the case that $n_1=n_2$.

## Going beyond Cohen's d

Cohen's $d$ might look like a sensible way to measure differences between means. For example, it captures the fact that 15351 has a larger $d$ number (a bigger effect) than 16854, corresponding to the fact that the means between words and pseudowords in the former case are more apart than in the latter case. Similarly, 170373 has a larger $d$ than 16854, corresponding to the fact that the former has a much smaller spread of the data (as seen in its standard deviation) than the latter.

However, there's something that should make you uneasy about using $d$ as a sensible proxy to answer the question of whether words and pseudowords affect RTs. Namely, we do not take into account how many data samples we collected. This is important if we want to learn something about populations, not just our samples. Clearly, if we collected more data from our population, that should weigh more than collecting fewer data. But this is not the case for Cohen's $d$. For that, notice that the last data set (pseudoword/word\_15292) with only 21 data points per group has almost the same $d$ as the full complete dataset. We turn to t-values to address this issue.

## T-values

The t-values are just like Cohen's $d$. Unlike Cohen's $d$ they do not tell us about differences of sample means, but differences in population means.

First, $t$ is calculated as (caveat: this calculation works for our simple case we consider here; for other cases, e.g., with paired observations or with one sample, the calculation is different - see also the lectures and the reading material):

$$t=\frac{\bar{x_1} - \bar{x_2}}{SE}$$

You can see that we standardize the distance between two means by SE, the standard error.
SE is calculated as shown below, where $n_1$ is the number of observations in group1 (i.e., the number of observations in $x_1$) and $n_2$ in group2. You can simplify this assuming that both groups are of equal size because we will work with equal size groups.

$$SE=\sqrt{\frac{var(x_1)}{n_1} + \frac{var(x_2)}{n_2}}$$

Another way to understand SE is to derive it from $s$ in Cohen's $d$ as shown below, where $s$ is calculated the same as in Cohen's $d$ and we divide by the square root of sizes of $x_1$ and $x_2$. So in other words we take Cohen's $d$ and adjust by the sizes of the samples. Again, this can be simplified if $n_1$ = $n_2$, as is the case in all cases below.

$$SE=s*\sqrt{1/n_1+1/n_2}$$

### Q2: Implement Cohen's d and the calculation of t values

In answering this question, you are not allowed to use any R packages or R functions that implement Cohen's d or the calculation of t-tests. However, you can use such packages (for example, effsize for Cohen's d and t.test for t-values) to double-check that your function works correctly. In doing so, be careful - some implementations might slightly differ wrt how they calculate $s$, so you might not get exactly identical numbers. This is another reason why you should not just rely on another package in your answer. If you check, say, stackoverflow or use an AI tool like ChatGPT, you should also be careful -- there are various ways of implementing Cohen's $d$ and $t$ calculation and not all are equivalent to what we do here.

First, implement Cohen's $d$ as a function in R. That is, you have to fill in the body of the function (what is put in as...) that you have here below. As said above, it is enough to implement the simplified version (one in which the length of $x_1$ and $x_2$ is the same).

```{r}

# cohend function with the help of CHATGPT
cohend <- function(x1, x2) {
  mean_x1 <- mean(x1)
  mean_x2 <- mean(x2)
  pooled_sd <- sqrt((var(x1) + var(x2)) / 2)
  d <- (mean_x1 - mean_x2) / pooled_sd
  return(d)

}
```

After the implementation, test your function and report collected Cohen's $d$ on four cases discussed below. Along with that, report whether the effect size is small, medium or large ($|d|<0.5$ is small, $|d|<0.8$ is medium, above that is large).
    
1. RTs for words and pseudowords for Subject numbered 15351.
2. RTs for words and pseudowords for Subject numbered 16854.
3. RTs for words and pseudowords for Subject numbered 170373.
4. RTs for all words and pseudowords.
5. RTs for the two vectors provided below as word\_15292 and pseudoword\_15292 (these are a few selected responses to words and pseudowords from subject 15292).

```{r}

subject_number <- 15351

# Get the response time for the word and pseudowords
words_rt <- merged_data %>%
  filter(Subject == subject_number, IsWord == TRUE) %>%
  pull(RT)

pseudowords_rt <- merged_data %>%
  filter(Subject == subject_number, IsWord == FALSE) %>%
  pull(RT)

effect_size_classification <- function(d) {
  if(abs(d) < 0.5) {
    return("small")
  } else if(abs(d) < 0.8) {
    return("medium")
  } else {
    return("large")
  }
}

# Calculate Cohen's d
d_value <- cohend(words_rt, pseudowords_rt)
effect_size <- effect_size_classification(d_value)

# Print results
print(paste("Cohen's d for subject", subject_number, "is", d_value, ". Effect size is", effect_size, "."))



word_15292 <- c(2206, 1583, 1154, 1010,  865,  931, 1129,  683,  820, 1132, 1049, 1211, 1261, 957, 1058,  790,  851, 1908, 1504, 1400,  924)

pseudoword_15292 <- c(677,  949,  889,  881,  917,  769,  772,  922, 1944,  881,  976, 1087, 1252,  914, 1277,  825, 1295, 1336,  788,  885,  932)

```

Finally, implement the t-calculation as a function. That is, fill in the body of this function (the same limitation that applied to Cohen's d applies here -- you cannot use other packages or R functions like t.test but you can consult AI tools or websites - and again, be careful if you use that, because there are different versions of t-tests):

```{r}

# T calculation function with the help of CHATGPT
tcalculation <- function(x1, x2) {
  n <- length(x1) # assuming n1 = n2
  mean_diff <- mean(x1) - mean(x2)
  pooled_sd <- sqrt((var(x1) + var(x2)) / 2)
  SE <- pooled_sd / sqrt(n)
  t_value <- mean_diff / SE
  return(t_value)

}

t_value <- cohend(words_rt, pseudowords_rt)
t_value

```

Once the implementation is done, calculate $t$ for:

1. RTs for words and pseudowords for Subject numbered 15351.
2. RTs for all words and pseudowords.
3. RTs for the two vectors provided below as word\_15292 and pseudoword\_15292 (these are a few selected responses to words and pseudowords from subject 15292).

Report the t-values and say briefly why RTs for all words and pseudowords (the second question above) have the highest t-value compared to pseudoword/word\_15292 and compared to the responses of Subject numbered 15351, and why this is not so for Cohen's d. A brief description of the crucial intuition suffices.

## Using the t-distribution to report p-values

In the Null Hypothesis Significance Testing (NHST), we study how likely it is that our results or a more extreme version of our results would have been observed under the null hypothesis. We will now assume that the null hypothesis is that the mean of the population from which $x_1$ is sampled does not differ from the mean of the population from which $x_2$ is sampled. What would then be the $p$ value?

This is the point where using t-values comes in useful. The t-values are accompanied by a probability distribution, so-called $t$-distribution. $t$-distribution with one parameter, degrees of freedom (df) $n$ expresses the probability that we would get such and such t-value if the data we were observing were normally distributed, the population mean was 0 and we sampled $n+1$ data.

Let us convince ourselves that the point in the last paragraph is correct. We will do so by running a small simulation.

First, we create a function that will sample 20 random data points from a normal distribution with mean 0 and standard deviation 10 (the size of the standard deviation is not important for this simulation). This random sampling is done using the function *rnorm*. Then, we calculate the t-value for that sample, using the definitions above but assuming just a single sample (that is, we do not compare two samples but do a one-sample t-test; see also the lecture and reading materials; we measure the difference of the mean of the current sample from 0, which is just the same as having the mean of the current sample in the numerator).

```{r}

generate.t <- function() {
    
    mysample <- rnorm(20, mean=0, sd=10)
    tvalue <- mean(mysample)/sqrt((var(mysample)/20))
    tvalue

}

```

To double check that our generated t-values correspond to probability distribution $t$, we can run a simulation and compare the results of the simulation to $t$ distribution. We follow the following steps:

1. We run simulation for many times, say 200,000 times. In each simulation, we sample from the same normal distribution and store the t-value.
2. Then, we compare the collected t-values to the theoretical probability distribution $t$ with degrees of freedom (df) = 19. Ideally, we should see a match. We first check a qqplot (check qqplot help on what it does) - we expect the simulated data and the theoretical distribution to fall on a line. This is roughly the case even though extreme values at both ends might slightly fall out (due to sampling). (This plot is not generated here because some pdf readers have problems to render all 200,000 points. We ran it separately and included the png file in the pdf file.) We also expect that in a histogram, we should see the same values in the simulated data and in the t-distribution.


```{r}

simulated_data <- rep(NA, 200000)

for (i in 1:length(simulated_data)) {

    simulated_data[i] <- generate.t()

}


# Q-Q plot
# qqplot(qt(ppoints(200000), df=19), simulated_data)
# qqline(simulated_data, distribution= function(p) qt(p, df=19))
# The code is not run, the resulting figure is just pasted here.

```

![image](figures/qqplot-simulated-t-test.png)

```{r}
# Histograms comparing t-values from simulated data and predicted based on the t-probability distribution
par(mfrow=c(2,1))
hist(simulated_data)
hist(qt(ppoints(200000), df=19))
# qt is a quantile function for t distribution. Its histogram should match the histogram of simulated data.

```

Indeed, we see a very good match between simulated data from which $t$ is collected and what we would expect to get just by looking at the theoretical $t$-distribution.

We can also see a very good match using the code right below. This code calculates the probability of getting $t$ value 1 or smaller using the theoretical distribution, and compares that to the frequency of cases that have the $t$ value 1 or smaller in our simulated data. We see a very good match between our simulation and the $t$ probability distribution.

```{r}

pt(1, df=19)

length(which(simulated_data <= 1))/length(simulated_data)

```


### Q3: transforming data and collecting p-values

Based on what we said so far, you should be able to tie t-values that you provided in Q2 to p-values under the null hypothesis that population means between RTs of words and RTs for pseudowords do not differ, i.e., mean(wordRT)=mean(pseudowordRT). Use the t-value from Q2 for the data set word\_15292 and pseudoword\_15292 and use the function *pt* (with degrees of freedom = 40) to provide the answer.

When you are done, come back to one of the assumptions of $t$-probability distributions: t-values are collected from samples of *independent and identically normally distributed data*. We focus on the latter condition. Check if RTs in words and pseudowords are normally distributed. 

If not, try a transformation to get closer to normal distribution. Among transformations, it is common to consider squaring, cubing, taking an inverse, taking square root, or log-transforming data. It is fine if you find only a roughly normal distribution (no testing needed, just checking by observing a histogram is sufficient for this exercise). 

Once you find the best case of transformation, report t-values and p-values for this transformed distribution. You can decide whether you want to use one-tailed or two-tailed tests but whatever you decide, report that.

### Q4: aggregating data

Even if we get a normal distribution of underlying data, we still did not address the issue of independence. Are all RTs in our data set independent?
Clearly not. Participants tend to differ in reaction times from each other and so there will be a dependence in reaction times of each participant. The way to avoid it is to not work with raw data but aggregations. 

Commonly when running a t-test on experimental data, we aggregate the dependent variable, e.g., RTs for words, per participant (that is, we get just one measure per participant, its mean RT over words). We do the same for pseudowords. Then, we calculate the t-value over these aggregated measures and then we calculate p-values. 

Do this for the dataset and report the results. Be careful in thinking about the type of t-test. Is this paired or unpaired?

### Q5: reading about an experiment

*The following question has to be answered individually. Each of you submits your own answer on Blackboard.*

Imagine you read about an auditory lexical decision task experiment. The experiment says that there were 20 words and 20 pseudowords tested. 30 participants took part in the experiment. Each participant saw all the words and pseudowords. Now, the paper says: ``We found a significant effect of word/pseudoword manipulation ($t=3.594, df=1198, p=0.00034$).'' Looking at this reported results, how did the researcher carry out his/her analysis (on subject-aggregated data, on non-aggregated data), and would you say that this was justified?

### Q6: a pitfall for p-values

Above, we calculated $p$ values based on the assumption that there are 40 degrees of freedom, corresponding to the collection of 42 data points (21 for word\_15292 and 21 for pseudoword\_15292; for each group the degrees of freedom are 21-1, which makes 40 degrees of freedom in total). In a way, we assume that the amount of data points were fixed and it was only open what the values of the data points was.

However, it is quite common that researchers do not know in advance how many participants they want to collect. 
Imagine the following situation: we decided we would be collecting data for the whole day and then we will stop and check the results. It happens so that on that day, there was a 50\% probability that we would collect 12 responses (6 for words, 6 for pseudowords) and a 50\% probability that we would collect 42 data points (21 for words and 21 for pseudowords). In our actual sample, we happened to collect the latter amount (i.e., 42 data points) and we got results as shown in word\_15292 and  pseudoword\_15292. What would \emph{then} be the p-value?

This is probably the most challenging question in this exercise. Here is a hint how to approach it: above (in Section *Using the t-distribution to report p-values*), we saw that you can approximate $t$ distribution using simulation. However, in the case above, we only approximated a $t$ distribution for a single sample, which must have consisted of 20 data points. Here, the situation is more complex for two reasons: (i) we collect two samples and calculate t-values by comparing them, (ii) it is given that the size of the samples is either 12 responses or 42 responses. This will complicate the simulation, but once you create it, you can read off the p-value from it just as we did above.

If you cannot calculate the value, try to at least reason about this: do you think that the p-value will be smaller than in Q4? Or will it be greater? In any case, note one very unintuitive aspect of p-values: they are dependent on experimenters' intentions and hypothetical situations (which might often not be explicitly stated, and might not even be considered!).

### Q7: an experiment where t-tests work

We raised various concerns regarding t-tests. We see that t-tests rely on independence across responses, and they assume that responses are distributed normally. The responses all have to follow the same normal distribution. And we also saw that $p$ values are hard to interpret, they depend on (often unexpressed) intentions of the researcher.

But are there experiments for which t-tests are a great fit?

In this question, try to think of an ``experiment'' data which could appropriately be analyzed by a t-test without any data aggregation. That is, raw data are already suitable for a t-test analysis. Try to describe the experiment in as much detail as possible: what conditions would there be, what variables, which variable would function as a predictor, which should be the outcome variable? Argue why you think t-tests are suitable here (in particular, why you think the data are normal, independent, identically distributed). The word experiment is put in quotes, since you can think of an experiment very broadly - observational studies (like checking a property of houses in a town, or checking some property of students in a class) also qualifies as an experiment.

## Linear models

*This part is easier to answer in the second week of the labs devoted to Assignment 1.*

So far, we worked all the time with RTs split by only one condition: IsWord. In fact, we can study more than one condition. For that, we have to turn to linear regression models. First, consider the following simple model, which only looks at the regression line based on IsWord.

```{r, eval=FALSE}

m1 <- lm(RT ~ IsWord, ...)#put in your data here
print(summary(m1))

```

We can add more parameters and study how they affect the regression line that predicts RTs. For example, the following model would consider the effect of IsWord, Accuracy and their interaction. The * in the notation calculates the main effect of both factors and the interaction of the two factors.

```{r, eval=FALSE}

m2 <- lm(RT ~ IsWord * ACC, ...)#put in your data here
print(summary(m2))

# The * sign signals interaction. See ?formula for details.

```

### Q8: Linear models and graphical representation

Run the models above (m1 and m2). Report results of both of them.

Then, provide a graphical summary accompanying m2. The summary should show how Accuracy and IsWord affect RTs. In your plot we should be able to see differences in the outcomes when the values of the predictor changes. That is, we should be able to see that RTs change when the values of IsWord change, and that RTs change when the values of ACC change, and that IsWord and ACC interact in affecting RTs.

### Q9: What frequency is the best predictor?

There are various sources of frequency in our dataset: FreCOCA, FreqGoogle, FreqSUBTLEX and FreqCOCAspok. These are frequencies of words collected from four different corpora. Find out which of these provides the best fit of the model to the log-transformed reaction times. You can do so by comparing models in which different frequency sources are added, or by comparing how big a proportion of the variance is explained by the model. 

After you find the answer, use the same frequency to address the following observation: it has been claimed that log-frequency of a word is a good predictor, better than a plain frequency, for log-reaction times. Is this correct? Plot the relation between the outcome variable and the non-transformed predictor variable to see whether any clear relation can be observed and whether the relation looks linear. Then, transform the frequency to log and plot again. Then, check the resulting model.

Finally, check whether the log-transformation of all frequency data sets changes your previous answer. Which corpus of frequency is now the best predictor when we consider its log-transformation?

### Q10: Experiment design

You are asked to design an experiment that will test whether log frequency or plain frequency is the right predictor of log reaction times (so, you basically want to test your answer to Q9, but rather than using a corpus data, you want to test your answer in a novel experiment). Your experiment should be a lexical decision task. What will the design be like? Describe (i) the conditions in your experiments, (ii) at least two items in your experiment (these should be words used in the lexical decision task), (iii) what fillers and control items you would use, (iv) how many stimuli would the whole experiment consist of, (v) what log-frequency predicts and what plain frequency predicts as results in your experiment.

## Bibliography

Tucker, Benjamin V, Daniel Brenner, D Kyle Danielson, Matthew C Kelley, Filip Nenadic, and Michelle Sims. 2019.
_The massive auditory lexical decision (MALD) database._ Behavior research methods 51:1187--1204.

